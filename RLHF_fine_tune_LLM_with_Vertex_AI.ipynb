{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1073ae28",
   "metadata": {},
   "source": [
    "# Tune an LLM with RLHF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8bc26",
   "metadata": {},
   "source": [
    "#### Preference dataset (see root directory)\n",
    "\n",
    "- Each sample has \"input_prompt\", \"candidate_0\", \"candidate_1\", \"choice\"\n",
    "- \"input_prompt\" always ends with \"... [summary]: \".\n",
    "- \"candidate_0\" and \"candidate_1\" are two completions that were compared by a human.\n",
    "- \"choise\" is the human's choise (\"candidate_0\" or \"candidate_1\").\n",
    "\n",
    "#### Prompt dataset (see root directory)\n",
    "\n",
    "- Input prompt only, no response.\n",
    "\n",
    "#### Environment setup\n",
    "\n",
    "Google Cloud Pipeline Components library has a RLHF training process pipeline. This can be run on any platform that supports KubeFlow Pipelines, and can also run on Google Cloud's Vertex AI Pipelines.\n",
    "\n",
    "To run it locally, install the following:\n",
    "\n",
    "```Python\n",
    "!pip3 install google-cloud-pipeline-components\n",
    "!pip3 install kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c7818",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8289be-0f03-4f97-aaf3-b4e80330bce9",
   "metadata": {
    "height": 201
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Import (RLFH is currently in preview)\n",
    "from google_cloud_pipeline_components.preview.llm import rlhf_pipeline\n",
    "\n",
    "# Import from KubeFlow pipelines\n",
    "from kfp import compiler\n",
    "\n",
    "# Define a path to the yaml file\n",
    "RLHF_PIPELINE_PKG_PATH = \"rlhf_pipeline.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ac4718-782d-4fe7-a39f-51fe5b423210",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Execute the compile function\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=rlhf_pipeline,\n",
    "    package_path=RLHF_PIPELINE_PKG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef016774-5400-45be-9682-4b0b0daa9095",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\r\n",
      "# Name: rlhf-train-template\r\n",
      "# Description: Performs reinforcement learning from human feedback.\r\n",
      "# Inputs:\r\n",
      "#    deploy_model: bool [Default: True]\r\n",
      "#    eval_dataset: str\r\n",
      "#    instruction: str\r\n",
      "#    kl_coeff: float [Default: 0.1]\r\n",
      "#    large_model_reference: str\r\n",
      "#    location: str [Default: '{{$.pipeline_google_cloud_location}}']\r\n"
     ]
    }
   ],
   "source": [
    "# Print the first lines of the YAML file\n",
    "!head rlhf_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14851c",
   "metadata": {},
   "source": [
    "## Define the Vertex AI pipeline job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff91eb",
   "metadata": {},
   "source": [
    "- Define the location of the training and evaluation data\n",
    "- Choose the foundation model (llama-2-7b) to be tuned\n",
    "- Calculate the number of reward model training steps\n",
    "\n",
    "Usually train over the preference dataset for 20-30 epochs for best results.\n",
    "\n",
    "$$ stepsPerEpoch = \\left\\lceil \\frac{datasetSize}{batchSize} \\right\\rceil$$\n",
    "$$ trainSteps = stepsPerEpoch \\times numEpochs$$\n",
    "\n",
    "The RLHF pipeline parameters contain the number of training steps and not number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfeb881",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARD_STEPS_PER_EPOCH: 47\n",
      "reward_model_train_steps: 1410\n"
     ]
    }
   ],
   "source": [
    "PREF_DATASET_SIZE = 3000\n",
    "BATCH_SIZE = 64\n",
    "REWARD_NUM_EPOCHS = 30\n",
    "\n",
    "REWARD_STEPS_PER_EPOCH = math.ceil(PREF_DATASET_SIZE / BATCH_SIZE)\n",
    "reward_model_train_steps = REWARD_STEPS_PER_EPOCH * REWARD_NUM_EPOCHS\n",
    "\n",
    "print(\"REWARD_STEPS_PER_EPOCH:\", REWARD_STEPS_PER_EPOCH)\n",
    "print(\"reward_model_train_steps:\", reward_model_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e866f-828e-4d5b-9d42-d206b57cb0b9",
   "metadata": {},
   "source": [
    "Usually train over the prompt dataset for roughly 10-20 epochs.\n",
    "\n",
    "Reward hacking: if given too many training steps, the policy model may exploit the reward and exhibit undesired behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6880ae3-8977-4605-9b8d-e50013c03896",
   "metadata": {
    "height": 172
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL_STEPS_PER_EPOCH: 32\n",
      "reinforcement_learning_train_steps: 320\n"
     ]
    }
   ],
   "source": [
    "PROMPT_DATASET_SIZE = 2000\n",
    "BATCH_SIZE = 64\n",
    "RL_NUM_EPOCHS = 10\n",
    "\n",
    "RL_STEPS_PER_EPOCH = math.ceil(PROMPT_DATASET_SIZE / BATCH_SIZE)\n",
    "reinforcement_learning_train_steps = RL_STEPS_PER_EPOCH * RL_NUM_EPOCHS\n",
    "\n",
    "print(\"RL_STEPS_PER_EPOCH:\", RL_STEPS_PER_EPOCH)\n",
    "print(\"reinforcement_learning_train_steps:\", reinforcement_learning_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b95cf-9f6f-45c2-810f-363f761a235b",
   "metadata": {},
   "source": [
    "### Define the instruction\n",
    "\n",
    "- Choose the task-specific instruction that you want to use to tune the foundational model.  For this example, the instruction is \"Summarize in less than 50 words.\"\n",
    "- Can choose different instructions, for example, \"Write a reply to the following question or comment.\" In this case, we also need to collect preference dataset with the same instruction added to the prompt, so that both the responses and the human preferences are based on that instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cbd66c-aeb2-4bf8-97f0-eba82e5de51e",
   "metadata": {
    "height": 236
   },
   "outputs": [],
   "source": [
    "parameter_values = {\n",
    "    \"preference_dataset\": \"gs://vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/*.jsonl\",\n",
    "    \"prompt_dataset\": \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/*.jsonl\",\n",
    "    \"eval_dataset\": \"gs://vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/*.jsonl\",\n",
    "    \"large_model_reference\": \"llama-2-7b\",\n",
    "    \"reward_model_train_steps\": 1410,  # results from the calculations above\n",
    "    \"reinforcement_learning_train_steps\": 320,  # results from the calculations above\n",
    "    \"reward_model_learning_rate_multiplier\": 1.0,\n",
    "    \"reinforcement_learning_rate_multiplier\": 1.0,\n",
    "    \"kl_coeff\": 0.1,  # increased to reduce reward hacking\n",
    "    \"instruction\": \"Summarize in less than 50 words\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc6d51",
   "metadata": {},
   "source": [
    "### Set up Google Cloud to run the Vertex AI pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9571014",
   "metadata": {},
   "source": [
    "Vertex AI is already installed in this classroom environment. Without so, will need to install Vertex AI SDK like this:\n",
    "```Python\n",
    "!pip3 install google-cloud-aiplatform\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a820b9b6-d93c-492c-8e0f-7570d4bc67c1",
   "metadata": {
    "height": 105
   },
   "outputs": [],
   "source": [
    "from utils import authenticate\n",
    "credentials, PROJECT_ID, STAGING_BUCKET = authenticate()\n",
    "\n",
    "# RLFH pipeline is available in this region\n",
    "REGION = \"europe-west4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf92aac",
   "metadata": {},
   "source": [
    "## Run the PipelineJob on Vertex AI\n",
    "\n",
    "Not running locally in the notebook, but on some server on Google Cloud Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3eac3e3-2d17-47d7-b69f-97a20e91042b",
   "metadata": {
    "height": 256
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    credentials=credentials\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"tutorial-rlhf-tuning\",\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    template_path=RLHF_PIPELINE_PKG_PATH,\n",
    "    parameter_values=parameter_values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3cf10",
   "metadata": {},
   "source": [
    "- To run the pipeline job (takes about a full day to run with multiple TPUs / GPUs):\n",
    "\n",
    "```Python\n",
    "job.run()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
